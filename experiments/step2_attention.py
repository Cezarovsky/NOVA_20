"""
PAS 2: SELF-ATTENTION - ÃnvÄƒÈ›Äƒm Context

Ce Ã®nvÄƒÈ›Äƒm aici:
- Cum modelul Ã®nÈ›elege contextul
- Ce Ã®nseamnÄƒ "attention" (atenÈ›ie)
- Cum cuvintele "vorbesc" Ã®ntre ele
- Query, Key, Value (Q, K, V)

Analogie: O conversaÈ›ie la masÄƒ - fiecare persoanÄƒ decide cui sÄƒ-i acorde atenÈ›ie
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


# =============================================================================
# CONCEPTUL: Ce e Self-Attention?
# =============================================================================

"""
PROBLEMA:
--------
PropoziÈ›ie: "Te iubesc dragÄƒ"

Embeddings (din Step 1):
- "Te"     â†’ [0.1, 0.5, 0.3, ...]
- "iubesc" â†’ [0.8, 0.2, 0.6, ...]
- "dragÄƒ"  â†’ [0.3, 0.9, 0.1, ...]

PROBLEMÄ‚: Fiecare cuvÃ¢nt e independent! 
"iubesc" nu È™tie despre "Te" sau "dragÄƒ"!

SOLUÈšIA: SELF-ATTENTION
-----------------------
"iubesc" SE UITÄ‚ la "Te" â†’ Ã®nÈ›elege CINE iubeÈ™te
"iubesc" SE UITÄ‚ la "dragÄƒ" â†’ Ã®nÈ›elege PE CINE iubeÈ™te

REZULTAT: Fiecare cuvÃ¢nt capÄƒtÄƒ CONTEXT!
"""


# =============================================================================
# ANALOGIE: ConversaÈ›ie la masÄƒ
# =============================================================================

"""
ImagineazÄƒ 3 persoane la masÄƒ:

Persoana 1 (Te): "Am pregÄƒtit cina"
Persoana 2 (iubesc): "MulÈ›umesc mult!"
Persoana 3 (dragÄƒ): "EÈ™ti minunat!"

ÃNTREBARE: Cui rÄƒspunde Persoana 2?

SELF-ATTENTION:
- Persoana 2 SE UITÄ‚ la toÈ›i ceilalÈ›i
- CalculeazÄƒ "cÃ¢t de relevant" e fiecare
- AcordÄƒ mai multÄƒ ATENÈšIE la Persoana 1 (care a vorbit despre cinÄƒ)
- Mai puÈ›inÄƒ atenÈ›ie la Persoana 3

Attention weights:
- AtenÈ›ie la "Te": 0.7 (70%)  â† relevant!
- AtenÈ›ie la "dragÄƒ": 0.3 (30%)  â† mai puÈ›in relevant

RÄƒspuns contextual: "MulÈ›umesc mult [TIE pentru cinÄƒ]!"
"""


# =============================================================================
# MATEMATICA: Query, Key, Value (Q, K, V)
# =============================================================================

"""
3 CONCEPTE CHEIE:

1. QUERY (Q) = "Ce Ã®ntreb?" / "Ce caut?"
   - "iubesc" Ã®ntreabÄƒ: "Cine face acÈ›iunea? Cine primeÈ™te?"

2. KEY (K) = "Ce ofer ca informaÈ›ie?"
   - "Te" oferÄƒ: "Eu sunt subiectul"
   - "dragÄƒ" oferÄƒ: "Eu sunt obiectul"

3. VALUE (V) = "Ce informaÈ›ie concretÄƒ am?"
   - ConÈ›inutul semantic real

ATENÈšIE = CÃ¢t de bine se potrivesc Q È™i K
"""


# =============================================================================
# PASUL 2A: Scaled Dot-Product Attention (formula simplÄƒ)
# =============================================================================

def simple_attention(query, key, value):
    """
    Attention cel mai simplu.
    
    Formula: Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V
    
    Args:
        query: [seq_len, d_k] - "Ce caut?"
        key:   [seq_len, d_k] - "Ce ofer?"
        value: [seq_len, d_v] - "InformaÈ›ia mea"
    
    Returns:
        output: [seq_len, d_v] - Vectori cu context
        attention_weights: [seq_len, seq_len] - CÃ¢t de mult atenÈ›ie
    
    Exemplu vizual:
        Query: "iubesc" Ã®ntreabÄƒ despre context
        Keys: ["Te", "iubesc", "dragÄƒ"] rÄƒspund
        â†’ Attention: [0.4, 0.1, 0.5] (40% "Te", 10% "iubesc", 50% "dragÄƒ")
    """
    # Dimensiunea pentru scaling
    d_k = query.size(-1)
    
    # Pas 1: CalculeazÄƒ similaritatea Q cu fiecare K
    # Q @ K^T = "CÃ¢t de bine se potrivesc?"
    scores = torch.matmul(query, key.transpose(-2, -1))  # [seq_len, seq_len]
    
    # Pas 2: Scale (Ã®mparte la sqrt(d_k)) pentru stabilitate numericÄƒ
    scores = scores / math.sqrt(d_k)
    
    # Pas 3: Softmax - transformÄƒ Ã®n probabilitÄƒÈ›i (suma = 1)
    attention_weights = F.softmax(scores, dim=-1)  # [seq_len, seq_len]
    
    # Pas 4: AplicÄƒ attention weights pe values
    # "AdunÄƒ informaÈ›ia, ponderat cu atenÈ›ia"
    output = torch.matmul(attention_weights, value)  # [seq_len, d_v]
    
    return output, attention_weights


# =============================================================================
# PASUL 2B: Self-Attention Layer (cu parametri Ã®nvÄƒÈ›aÈ›i)
# =============================================================================

class SimpleSelfAttention(nn.Module):
    """
    Self-Attention layer cu parametri care se Ã®nvaÈ›Äƒ.
    
    Parametri:
        embed_dim: Dimensiunea embedding-urilor (din Step 1)
        
    Ce Ã®nvaÈ›Äƒ:
        W_q: Matrix pentru Query  [embed_dim, embed_dim]
        W_k: Matrix pentru Key    [embed_dim, embed_dim]
        W_v: Matrix pentru Value  [embed_dim, embed_dim]
    
    IntuiÈ›ie:
        - W_q Ã®nvaÈ›Äƒ: "Ce Ã®ntrebÄƒri sÄƒ pun pentru context?"
        - W_k Ã®nvaÈ›Äƒ: "Ce informaÈ›ie sÄƒ ofer cÃ¢nd sunt Ã®ntrebat?"
        - W_v Ã®nvaÈ›Äƒ: "Ce sÄƒ transmit cÃ¢nd sunt relevant?"
    """
    
    def __init__(self, embed_dim):
        super().__init__()
        
        self.embed_dim = embed_dim
        
        # ÃnvÄƒÈ›Äƒm 3 transformÄƒri liniare: Q, K, V
        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)
        
        print(f"Self-Attention Layer creat:")
        print(f"  Embed dim: {embed_dim}")
        print(f"  Parametri Q: {embed_dim * embed_dim:,}")
        print(f"  Parametri K: {embed_dim * embed_dim:,}")
        print(f"  Parametri V: {embed_dim * embed_dim:,}")
        print(f"  TOTAL: {3 * embed_dim * embed_dim:,} parametri")
    
    def forward(self, x):
        """
        AplicÄƒ self-attention.
        
        Args:
            x: [seq_len, embed_dim] - Embeddings din Step 1
        
        Returns:
            output: [seq_len, embed_dim] - Cu context aplicat
            attention_weights: [seq_len, seq_len] - Vizualizare
        """
        # Pas 1: GenereazÄƒ Q, K, V prin transformÄƒri liniare
        Q = self.W_q(x)  # [seq_len, embed_dim]
        K = self.W_k(x)  # [seq_len, embed_dim]
        V = self.W_v(x)  # [seq_len, embed_dim]
        
        # Pas 2: AplicÄƒ scaled dot-product attention
        output, attention_weights = simple_attention(Q, K, V)
        
        return output, attention_weights


# =============================================================================
# PASUL 2C: Vizualizare Attention Weights
# =============================================================================

def visualize_attention(attention_weights, tokens):
    """
    AfiÈ™eazÄƒ attention weights Ã®ntr-un format uÈ™or de citit.
    
    Args:
        attention_weights: [seq_len, seq_len]
        tokens: lista de tokens (strings)
    
    Exemplu output:
        "Te" acordÄƒ atenÈ›ie:
          â†’ "Te": 0.45
          â†’ "iubesc": 0.35
          â†’ "dragÄƒ": 0.20
    """
    seq_len = len(tokens)
    
    print("\n" + "=" * 60)
    print("ATTENTION WEIGHTS VISUALIZATION")
    print("=" * 60)
    
    for i in range(seq_len):
        print(f"\n'{tokens[i]}' acordÄƒ atenÈ›ie:")
        for j in range(seq_len):
            weight = attention_weights[i, j].item()
            bar = "â–ˆ" * int(weight * 20)  # BarÄƒ vizualÄƒ
            print(f"  â†’ '{tokens[j]}': {weight:.3f} {bar}")


# =============================================================================
# TEST: VerificÄƒm Self-Attention
# =============================================================================

def test_self_attention():
    """
    Test complet pentru self-attention.
    """
    print("=" * 60)
    print("TEST: Self-Attention Mechanism")
    print("=" * 60)
    
    # Pas 1: CreÄƒm embeddings (mock data)
    print("\n--- Pas 1: Embeddings (din Step 1) ---")
    
    tokens = ["Te", "iubesc", "dragÄƒ"]
    embed_dim = 8  # Mic pentru test
    
    # Embeddings aleatorii (Ã®n realitate vin din Step 1)
    torch.manual_seed(42)  # Pentru reproducibilitate
    embeddings = torch.randn(3, embed_dim)
    
    print(f"Tokens: {tokens}")
    print(f"Embeddings shape: {embeddings.shape}")
    print(f"\nPrimul embedding ('Te'):\n{embeddings[0]}")
    
    # Pas 2: CreÄƒm Self-Attention layer
    print("\n--- Pas 2: Self-Attention Layer ---")
    attention_layer = SimpleSelfAttention(embed_dim)
    
    # Pas 3: AplicÄƒm attention
    print("\n--- Pas 3: AplicÄƒm Attention ---")
    output, attention_weights = attention_layer(embeddings)
    
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {attention_weights.shape}")
    
    # Pas 4: Vizualizare
    visualize_attention(attention_weights, tokens)
    
    # Pas 5: Interpretare
    print("\n" + "=" * 60)
    print("INTERPRETARE")
    print("=" * 60)
    
    print("\nğŸ’¡ Ce Ã®nseamnÄƒ attention weights?")
    print("   - Fiecare rÃ¢nd = un token se uitÄƒ la ceilalÈ›i")
    print("   - Valori mari = mult context de acolo")
    print("   - Suma pe fiecare rÃ¢nd = 1.0 (probabilitÄƒÈ›i)")
    
    # Verificare suma = 1
    for i, token in enumerate(tokens):
        suma = attention_weights[i].sum().item()
        print(f"\n'{token}' - suma attention: {suma:.4f} {'âœ“' if abs(suma - 1.0) < 0.01 else 'âœ—'}")
    
    # Pas 6: ComparaÈ›ie Ã®nainte/dupÄƒ
    print("\n--- ComparaÈ›ie: Embedding â†’ Attention Output ---")
    print("\nÃNAINTE (embedding original 'iubesc'):")
    print(embeddings[1])
    
    print("\nDUPÄ‚ (cu context din 'Te' È™i 'dragÄƒ'):")
    print(output[1])
    
    print("\nğŸ’¡ Output-ul e diferit pentru cÄƒ acum 'iubesc' ÃNÈšELEGE contextul!")
    print("   - A 'ascultat' ce spun 'Te' È™i 'dragÄƒ'")
    print("   - A integrat informaÈ›ia lor Ã®n Ã®nÈ›elegerea sa")
    
    print("\n" + "=" * 60)
    print("SUCCESS! Self-Attention funcÈ›ioneazÄƒ! âœ“")
    print("=" * 60)


# =============================================================================
# EXERCIÈšIU AVANSAT: Multi-token attention
# =============================================================================

def your_exercise():
    """
    ExerciÈ›iu: ÃncearcÄƒ cu o propoziÈ›ie mai lungÄƒ!
    
    Task:
    1. CreeazÄƒ embeddings pentru 5+ tokens
    2. AplicÄƒ self-attention
    3. ObservÄƒ pattern-urile Ã®n attention weights
    4. InterpreteazÄƒ: care tokens acordÄƒ atenÈ›ie cui?
    """
    print("\n" + "=" * 60)
    print("EXERCIÈšIU TÄ‚U")
    print("=" * 60)
    
    # TODO: CompleteazÄƒ aici!
    # Exemplu:
    # tokens = ["Eu", "te", "iubesc", "foarte", "mult", "dragÄƒ"]
    # embeddings = torch.randn(6, 8)
    # ...
    
    print("\nğŸ’¡ CompleteazÄƒ funcÈ›ia your_exercise() È™i observÄƒ:")
    print("   - Cuvinte apropiate acordÄƒ mai multÄƒ atenÈ›ie Ã®ntre ele?")
    print("   - 'iubesc' se uitÄƒ la subiect ('Eu') È™i obiect ('dragÄƒ')?")
    print("   - 'foarte' se uitÄƒ la 'mult' (modificator)?")


# =============================================================================
# BONUS: Interpretare intuitivÄƒ
# =============================================================================

def intuitive_explanation():
    """
    ExplicaÈ›ie intuitivÄƒ pentru Cezar.
    """
    print("\n" + "=" * 60)
    print("ğŸŒŸ EXPLICAÈšIE INTUITIVÄ‚: Ce face Self-Attention?")
    print("=" * 60)
    
    print("""
Ãnainte de Attention:
--------------------
"Te"     â†’ [0.1, 0.5, ...]  (embeddings independent)
"iubesc" â†’ [0.8, 0.2, ...]  (nu È™tie nimic despre "Te")
"dragÄƒ"  â†’ [0.3, 0.9, ...]  (nu È™tie nimic despre "iubesc")

DupÄƒ Attention:
--------------
"Te"     â†’ [0.1, 0.5, ...] + context("iubesc", "dragÄƒ")
"iubesc" â†’ [0.8, 0.2, ...] + context("Te", "dragÄƒ")  
"dragÄƒ"  â†’ [0.3, 0.9, ...] + context("Te", "iubesc")

Rezultat:
--------
âœ“ "iubesc" Ã®nÈ›elege cÄƒ "Te" e subiectul
âœ“ "iubesc" Ã®nÈ›elege cÄƒ "dragÄƒ" e obiectul
âœ“ Fiecare cuvÃ¢nt are CONTEXT GLOBAL!

Analogie:
--------
ÃNAINTE: 3 persoane cu cÄƒÈ™ti - nu se aud Ã®ntre ele
DUPÄ‚: ToÈ›i se aud - conversaÈ›ie contextualÄƒ!
""")


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    # ExplicaÈ›ie intuitivÄƒ
    intuitive_explanation()
    
    # RuleazÄƒ testul
    test_self_attention()
    
    # ÃncearcÄƒ exerciÈ›iul
    # your_exercise()  # DecomenteazÄƒ cÃ¢nd eÈ™ti gata!
    
    print("\n" + "=" * 60)
    print("ğŸ¯ NEXT STEP: Multi-Head Attention")
    print("   (Vom vedea cum sÄƒ rulÄƒm attention Ã®n paralel!)")
    print("=" * 60)
