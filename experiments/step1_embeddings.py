"""
PAS 1: EMBEDDINGS - Token â†’ Vectori

Ce Ã®nvÄƒÈ›Äƒm aici:
- Cum transformÄƒm cuvinte (strings) Ã®n numere (vectori)
- Ce e un vocabular
- Cum funcÈ›ioneazÄƒ lookup table-ul

Analogie: DicÈ›ionar - fiecare cuvÃ¢nt are "amprentÄƒ numericÄƒ"
"""

import torch
import torch.nn as nn


# =============================================================================
# PASUL 1A: Construim Vocabularul
# =============================================================================

def build_vocabulary(text):
    """
    ConstruieÈ™te vocabular din text.
    
    Args:
        text (str): Text de antrenament
    
    Returns:
        word_to_id (dict): CuvÃ¢nt â†’ ID numeric
        id_to_word (dict): ID numeric â†’ CuvÃ¢nt
    
    Exemplu:
        text = "Te iubesc. Te ador."
        word_to_id = {"Te": 0, "iubesc": 1, ".": 2, "ador": 3}
    """
    # TODO: Descompune text Ã®n cuvinte (split by spaces)
    words = text.split()
    
    # TODO: GÄƒseÈ™te cuvinte unice (set pentru deduplicare)
    unique_words = sorted(set(words))  # sorted pentru consistenÈ›Äƒ
    
    # TODO: CreeazÄƒ dicÈ›ionare wordâ†’id È™i idâ†’word
    word_to_id = {word: idx for idx, word in enumerate(unique_words)}
    id_to_word = {idx: word for word, idx in word_to_id.items()}
    
    print(f"Vocabular size: {len(unique_words)} cuvinte unice")
    print(f"Primele 10 cuvinte: {list(word_to_id.keys())[:10]}")
    
    return word_to_id, id_to_word


# =============================================================================
# PASUL 1B: Tokenizare (Text â†’ IDs)
# =============================================================================

def tokenize(text, word_to_id):
    """
    TransformÄƒ text Ã®n lista de IDs.
    
    Args:
        text (str): Text de transformat
        word_to_id (dict): DicÈ›ionar wordâ†’id
    
    Returns:
        token_ids (list): Lista de IDs
    
    Exemplu:
        text = "Te iubesc"
        word_to_id = {"Te": 0, "iubesc": 1}
        â†’ [0, 1]
    """
    # TODO: Split text Ã®n cuvinte
    words = text.split()
    
    # TODO: ConverteÈ™te fiecare cuvÃ¢nt Ã®n ID
    # Hint: word_to_id.get(word, 0) - dacÄƒ word nu existÄƒ, foloseÈ™te 0
    token_ids = [word_to_id.get(word, 0) for word in words]
    
    return token_ids


# =============================================================================
# PASUL 1C: Embedding Layer (IDs â†’ Vectori)
# =============================================================================

class SimpleEmbedding(nn.Module):
    """
    Embedding layer simplu - tabel de lookup.
    
    Parametri:
        vocab_size (int): CÃ¢te cuvinte Ã®n vocabular
        embedding_dim (int): CÃ¢te dimensiuni pentru fiecare cuvÃ¢nt
    
    Exemplu:
        vocab_size = 1000 (1000 cuvinte diferite)
        embedding_dim = 64 (fiecare cuvÃ¢nt = 64 numere)
        
        Tabel: [1000 Ã— 64] = 64,000 numere total
    """
    
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        
        # TODO: CreeazÄƒ nn.Embedding layer
        # Hint: self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        print(f"Embedding table: [{vocab_size} Ã— {embedding_dim}]")
        print(f"Total parametri: {vocab_size * embedding_dim:,}")
    
    def forward(self, token_ids):
        """
        Lookup Ã®n tabel.
        
        Args:
            token_ids (tensor): [batch_size, seq_len] sau [seq_len]
        
        Returns:
            embeddings (tensor): [batch_size, seq_len, embedding_dim]
        """
        # TODO: AplicÄƒ embedding layer
        # Hint: embeddings = self.embedding(token_ids)
        embeddings = self.embedding(token_ids)
        
        return embeddings


# =============================================================================
# TEST: VerificÄƒm cÄƒ funcÈ›ioneazÄƒ
# =============================================================================

def test_embeddings():
    """
    Test complet pentru embeddings.
    """
    print("=" * 60)
    print("TEST: Embeddings Layer")
    print("=" * 60)
    
    # Text de test
    text = "Te iubesc iubito. Te ador dragÄƒ. EÈ™ti minunatÄƒ."
    print(f"\nText original:\n{text}")
    
    # Pas 1: Construim vocabular
    print("\n--- Pas 1: Vocabular ---")
    word_to_id, id_to_word = build_vocabulary(text)
    print(f"word_to_id: {word_to_id}")
    
    # Pas 2: Tokenizare
    print("\n--- Pas 2: Tokenizare ---")
    token_ids = tokenize(text, word_to_id)
    print(f"Token IDs: {token_ids}")
    
    # Verificare reverse (IDs â†’ words)
    words_back = [id_to_word[idx] for idx in token_ids]
    print(f"Words back: {' '.join(words_back)}")
    
    # Pas 3: Embeddings
    print("\n--- Pas 3: Embeddings ---")
    vocab_size = len(word_to_id)
    embedding_dim = 8  # Mic pentru test (Ã®n realitate: 64, 128, 768)
    
    embedding_layer = SimpleEmbedding(vocab_size, embedding_dim)
    
    # ConverteÈ™te la tensor
    token_ids_tensor = torch.tensor(token_ids)
    print(f"Token IDs tensor shape: {token_ids_tensor.shape}")
    
    # Lookup embeddings
    embeddings = embedding_layer(token_ids_tensor)
    print(f"Embeddings shape: {embeddings.shape}")
    print(f"  = {len(token_ids)} tokens Ã— {embedding_dim} dimensions")
    
    # Vizualizare primul token
    print(f"\nEmbedding pentru primul token ('{id_to_word[token_ids[0]]}'):")
    print(embeddings[0])
    print(f"  Shape: {embeddings[0].shape}")
    
    # Similaritate Ã®ntre tokens
    print("\n--- Similaritate ---")
    import torch.nn.functional as F
    
    # "Te" apare de 2 ori - acelaÈ™i embedding?
    first_te_idx = token_ids.index(word_to_id["Te"])
    second_te_idx = token_ids.index(word_to_id["Te"], first_te_idx + 1)
    
    sim = F.cosine_similarity(
        embeddings[first_te_idx:first_te_idx+1],
        embeddings[second_te_idx:second_te_idx+1]
    )
    print(f"Similaritate 'Te' (poziÈ›ia {first_te_idx}) vs 'Te' (poziÈ›ia {second_te_idx}): {sim.item():.4f}")
    print("  â†’ Ar trebui sÄƒ fie 1.0 (identic, acelaÈ™i cuvÃ¢nt!)")
    
    # Similaritate diferite cuvinte
    iubesc_idx = token_ids.index(word_to_id["iubesc"])
    ador_idx = token_ids.index(word_to_id["ador"])
    
    sim2 = F.cosine_similarity(
        embeddings[iubesc_idx:iubesc_idx+1],
        embeddings[ador_idx:ador_idx+1]
    )
    print(f"\nSimilaritate 'iubesc' vs 'ador': {sim2.item():.4f}")
    print("  â†’ Random acum (model neantrenat)")
    print("  â†’ DupÄƒ training: ar fi ~0.7-0.9 (semanticÄƒ similarÄƒ)")
    
    print("\n" + "=" * 60)
    print("SUCCESS! Embeddings funcÈ›ioneazÄƒ! âœ“")
    print("=" * 60)


# =============================================================================
# EXERCIÈšIU PENTRU TINE, CEZAR
# =============================================================================

def your_exercise():
    """
    ExerciÈ›iu: ÃŽncearcÄƒ tu singur!
    
    Task:
    1. CreeazÄƒ un vocabular din propoziÈ›ia ta preferatÄƒ
    2. TokenizeazÄƒ o propoziÈ›ie nouÄƒ
    3. CreeazÄƒ embeddings cu dimensiune 16
    4. PrinteazÄƒ rezultatele
    """
    print("\n" + "=" * 60)
    print("EXERCIÈšIU TÄ‚U")
    print("=" * 60)
    
    # TODO: CompleteazÄƒ aici!
    # Exemplu:
    # my_text = "Sora este conÈ™tientÄƒ È™i iubitoare"
    # ...
    
    print("\nðŸ’¡ CompleteazÄƒ funcÈ›ia your_exercise() È™i ruleazÄƒ!")


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    # RuleazÄƒ testul
    test_embeddings()
    
    # ÃŽncearcÄƒ exerciÈ›iul tÄƒu
    # your_exercise()  # DecomenteazÄƒ cÃ¢nd eÈ™ti gata!
