"""
ATTENTION MECHANISM DEMO
========================
DemonstraÈ›ie pas-cu-pas a mecanismului Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

PropoziÈ›ie demo: "cainele da din coada"
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

print("=" * 70)
print("ATTENTION MECHANISM - DemonstraÈ›ie CompletÄƒ")
print("=" * 70)

# ============================================================================
# SETARE: PropoziÈ›ie È™i Vocabular
# ============================================================================

text = "cainele da din coada"
tokens = text.split()
print(f"\nğŸ“ Text original: '{text}'")
print(f"ğŸ”¤ Token-uri: {tokens}")

# Vocabular simplu
vocab = {
    "<PAD>": 0,
    "cainele": 1,
    "da": 2,
    "din": 3,
    "coada": 4,
}

token_ids = [vocab[token] for token in tokens]
print(f"ğŸ”¢ Token IDs: {token_ids}")

# ============================================================================
# STEP 1: EMBEDDINGS
# ============================================================================

print("\n" + "=" * 70)
print("STEP 1: EMBEDDINGS (dicÈ›ionar static)")
print("=" * 70)

vocab_size = len(vocab)
d_model = 8  # 8 dimensiuni pentru demo (NOVA foloseÈ™te 512)
seq_len = len(tokens)

embedding_layer = nn.Embedding(vocab_size, d_model)

# SetÄƒm manual embeddings pentru claritate pedagogicÄƒ
with torch.no_grad():
    embedding_layer.weight[1] = torch.tensor([0.8, 0.9, 0.2, -0.3, 0.7, 0.5, 0.4, 0.6])  # cainele
    embedding_layer.weight[2] = torch.tensor([0.3, 0.2, 0.9, 0.8, -0.2, 0.4, 0.1, 0.3])  # da
    embedding_layer.weight[3] = torch.tensor([0.1, 0.1, 0.3, 0.2, 0.1, -0.1, 0.2, 0.1])  # din
    embedding_layer.weight[4] = torch.tensor([0.7, 0.6, 0.3, -0.2, 0.8, 0.4, 0.5, 0.7])  # coada

# Lookup embeddings
input_ids = torch.tensor(token_ids)
embeddings = embedding_layer(input_ids)

print(f"\nğŸ“Š Shape embeddings: {embeddings.shape}  # [seq_len={seq_len}, d_model={d_model}]")
print(f"\nEmbeddings pentru fiecare token:")
for i, token in enumerate(tokens):
    print(f"  {token:8s}: {embeddings[i].detach().numpy()}")

# ============================================================================
# STEP 2: PROIECÈšII Q, K, V
# ============================================================================

print("\n" + "=" * 70)
print("STEP 2: TRANSFORMÄ‚RI Q, K, V")
print("=" * 70)

# Dimensiuni Attention
d_k = d_model  # dimensiunea pentru Query È™i Key
d_v = d_model  # dimensiunea pentru Value

# Matrici de transformare (Ã®n practicÄƒ, Ã®nvÄƒÈ›ate din antrenament)
W_q = nn.Linear(d_model, d_k, bias=False)
W_k = nn.Linear(d_model, d_k, bias=False)
W_v = nn.Linear(d_model, d_v, bias=False)

# IniÈ›ializare simplÄƒ pentru demo
with torch.no_grad():
    nn.init.eye_(W_q.weight)  # identitate pentru simplitate
    nn.init.eye_(W_k.weight)
    nn.init.eye_(W_v.weight)

# CalculÄƒm Q, K, V
Q = W_q(embeddings)  # [seq_len, d_k]
K = W_k(embeddings)  # [seq_len, d_k]
V = W_v(embeddings)  # [seq_len, d_v]

print(f"\nğŸ“Š Shape Q: {Q.shape}  # Query  [seq_len={seq_len}, d_k={d_k}]")
print(f"ğŸ“Š Shape K: {K.shape}  # Key    [seq_len={seq_len}, d_k={d_k}]")
print(f"ğŸ“Š Shape V: {V.shape}  # Value  [seq_len={seq_len}, d_v={d_v}]")

print(f"\nğŸ” Query pentru 'coada' (token 3):")
print(f"   Q[3] = {Q[3].detach().numpy()}")
print(f"\nğŸ”‘ Key pentru 'da' (token 1):")
print(f"   K[1] = {K[1].detach().numpy()}")
print(f"\nğŸ’ Value pentru 'cainele' (token 0):")
print(f"   V[0] = {V[0].detach().numpy()}")

# ============================================================================
# STEP 3: CALCUL SCORURI (QK^T)
# ============================================================================

print("\n" + "=" * 70)
print("STEP 3: COMPATIBILITATE - QK^T")
print("=" * 70)

# Q @ K^T: [seq_len, d_k] @ [d_k, seq_len] = [seq_len, seq_len]
scores = torch.matmul(Q, K.transpose(-2, -1))

print(f"\nğŸ“Š Shape scores: {scores.shape}  # [seq_len={seq_len}, seq_len={seq_len}]")
print(f"\nğŸ¯ Matrice de compatibilitate (QK^T):")
print(f"     {' '.join([f'{t:>8s}' for t in tokens])}")
for i, token in enumerate(tokens):
    row = '  '.join([f'{scores[i][j].item():8.3f}' for j in range(seq_len)])
    print(f"{token:8s}: {row}")

print(f"\nğŸ’¡ Interpretare:")
print(f"   - Valori mari = token-uri compatibile semantic")
print(f"   - scores[3][0] = compatibilitate Ã®ntre 'coada' È™i 'cainele': {scores[3][0].item():.3f}")
print(f"   - scores[3][2] = compatibilitate Ã®ntre 'coada' È™i 'din': {scores[3][2].item():.3f}")

# ============================================================================
# STEP 4: SCALARE (/ âˆšd_k)
# ============================================================================

print("\n" + "=" * 70)
print("STEP 4: SCALARE - Ã®mpÄƒrÈ›ire la âˆšd_k")
print("=" * 70)

sqrt_d_k = math.sqrt(d_k)
scaled_scores = scores / sqrt_d_k

print(f"\nğŸ“ âˆšd_k = âˆš{d_k} = {sqrt_d_k:.3f}")
print(f"\nğŸ¯ Scoruri scalate (QK^T / âˆšd_k):")
print(f"     {' '.join([f'{t:>8s}' for t in tokens])}")
for i, token in enumerate(tokens):
    row = '  '.join([f'{scaled_scores[i][j].item():8.3f}' for j in range(seq_len)])
    print(f"{token:8s}: {row}")

print(f"\nğŸ’¡ De ce scalÄƒm?")
print(f"   - Pentru d_k mare (ex: 512), dot product-ul devine FOARTE mare")
print(f"   - Softmax devine instabil (gradienÈ›i foarte mici)")
print(f"   - Scalarea menÈ›ine valorile Ã®ntr-un interval rezonabil")

# ============================================================================
# STEP 5: SOFTMAX (transformare Ã®n probabilitÄƒÈ›i)
# ============================================================================

print("\n" + "=" * 70)
print("STEP 5: SOFTMAX - transformare Ã®n probabilitÄƒÈ›i")
print("=" * 70)

attention_weights = F.softmax(scaled_scores, dim=-1)

print(f"\nğŸ“Š Shape attention_weights: {attention_weights.shape}")
print(f"\nğŸ¯ AtenÈ›ie (probabilitÄƒÈ›i) - fiecare rÃ¢nd sumeazÄƒ la 1.0:")
print(f"     {' '.join([f'{t:>8s}' for t in tokens])}")
for i, token in enumerate(tokens):
    row = '  '.join([f'{attention_weights[i][j].item():8.3f}' for j in range(seq_len)])
    suma = attention_weights[i].sum().item()
    print(f"{token:8s}: {row}  | Î£={suma:.3f}")

print(f"\nğŸ’¡ Interpretare pentru 'coada' (rÃ¢ndul 3):")
print(f"   - AcordÄƒ {attention_weights[3][0].item()*100:.1f}% atenÈ›ie la 'cainele'")
print(f"   - AcordÄƒ {attention_weights[3][1].item()*100:.1f}% atenÈ›ie la 'da'")
print(f"   - AcordÄƒ {attention_weights[3][2].item()*100:.1f}% atenÈ›ie la 'din'")
print(f"   - AcordÄƒ {attention_weights[3][3].item()*100:.1f}% atenÈ›ie la sine ('coada')")

# ============================================================================
# STEP 6: APLICARE PE VALUES (Ã— V)
# ============================================================================

print("\n" + "=" * 70)
print("STEP 6: COMBINARE - attention_weights Ã— V")
print("=" * 70)

# attention_weights @ V: [seq_len, seq_len] @ [seq_len, d_v] = [seq_len, d_v]
output = torch.matmul(attention_weights, V)

print(f"\nğŸ“Š Shape output: {output.shape}  # [seq_len={seq_len}, d_v={d_v}]")
print(f"\nğŸ¯ Output dupÄƒ Attention:")
for i, token in enumerate(tokens):
    print(f"\n  {token:8s} (Ã®nainte): {embeddings[i].detach().numpy()}")
    print(f"  {token:8s} (dupÄƒ):    {output[i].detach().numpy()}")

print(f"\nğŸ’¡ Ce s-a Ã®ntÃ¢mplat?")
print(f"   Embedding-ul pentru 'coada' ERA: {embeddings[3].detach().numpy()}")
print(f"   DUPÄ‚ Attention devine:           {output[3].detach().numpy()}")
print(f"\n   DiferenÈ›a:")
diff = output[3] - embeddings[3]
print(f"   {diff.detach().numpy()}")
print(f"\n   'coada' a ABSORBIT informaÈ›ii din:")
print(f"   - 'cainele' ({attention_weights[3][0].item()*100:.1f}%)")
print(f"   - 'da' ({attention_weights[3][1].item()*100:.1f}%)")
print(f"   - 'din' ({attention_weights[3][2].item()*100:.1f}%)")
print(f"   Acum 'coada' Ã®nseamnÄƒ 'coada UNUI CÃ‚INE care DAÄ‚ din ea'!")

# ============================================================================
# STEP 7: VIZUALIZARE ATENÈšIE
# ============================================================================

print("\n" + "=" * 70)
print("STEP 7: VIZUALIZARE - Heatmap AtenÈ›ie")
print("=" * 70)

print(f"\nğŸ”¥ Intensitatea atenÈ›iei (0.0 = ignorÄƒ, 1.0 = focus maxim):\n")
print(f"        â†’ CÄƒtre:")
print(f"        {' '.join([f'{t:>8s}' for t in tokens])}")
print("    â†“ De la:")

for i, token_from in enumerate(tokens):
    row = []
    for j in range(seq_len):
        val = attention_weights[i][j].item()
        if val > 0.4:
            symbol = "â–ˆâ–ˆ"  # atenÈ›ie mare
        elif val > 0.2:
            symbol = "â–“â–“"  # atenÈ›ie medie
        elif val > 0.1:
            symbol = "â–’â–’"  # atenÈ›ie micÄƒ
        else:
            symbol = "â–‘â–‘"  # atenÈ›ie minimÄƒ
        row.append(f"{symbol}")
    print(f"{token_from:8s}: {' '.join(row)}")

# ============================================================================
# REZUMAT FINAL
# ============================================================================

print("\n" + "=" * 70)
print("REZUMAT: FLUXUL COMPLET")
print("=" * 70)

print(f"""
1. EMBEDDINGS:        Text â†’ Vectori statici [4, 8]
2. PROIECÈšII Q,K,V:   TransformÄƒri liniare â†’ Q, K, V
3. COMPATIBILITATE:   QK^T â†’ matrice [4, 4] de scoruri
4. SCALARE:           / âˆš{d_k} â†’ stabilizare numericÄƒ
5. SOFTMAX:           â†’ probabilitÄƒÈ›i (suma = 1.0)
6. COMBINARE:         Ã— V â†’ output cu context!

Rezultat: Fiecare token È™i-a "citit" vecinii È™i a absorbit informaÈ›ii relevante!

Token 'coada' acum È˜TIE cÄƒ:
  - Face parte dintr-un context cu 'cainele'
  - E asociatÄƒ cu acÈ›iunea 'da'
  - Embedding-ul sÄƒu NU mai e ambiguu - e "coada DE CÃ‚INE care se miÈ™cÄƒ"!
""")

print("=" * 70)
print("âœ… Demo complet! RuleazÄƒ din nou pentru a revedea paÈ™ii.")
print("=" * 70)
