# üìñ NOVA AI System - Manual de Utilizare »ôi Implementare

**Project**: NOVA - Intelligent AI Assistant  
**Version**: 0.5.0-beta  
**Last Updated**: December 3, 2024  
**Status**: ‚úÖ Core Systems Operational

---

## üìã Cuprins

1. [Introducere](#1-introducere)
2. [Arhitectura Sistemului](#2-arhitectura-sistemului)
3. [Componente Implementate](#3-componente-implementate)
4. [Ghid de Instalare](#4-ghid-de-instalare)
5. [Ghid de Utilizare](#5-ghid-de-utilizare)
6. [API Reference](#6-api-reference)
7. [Exemple Practice](#7-exemple-practice)
8. [Depanare »ôi Troubleshooting](#8-depanare-»ôi-troubleshooting)
9. [Dezvoltare Viitoare](#9-dezvoltare-viitoare)

---

## 1. Introducere

### 1.1 Ce este NOVA?

NOVA este un sistem AI avansat cu urmƒÉtoarele capabilitƒÉ»õi:

- üß† **Transformer-based Language Model**: ArhitecturƒÉ completƒÉ cu attention mechanisms
- üéôÔ∏è **Voice Capabilities**: Text-to-Speech multilingual (rom√¢nƒÉ + englezƒÉ)
- üß† **RAG System**: Retrieval-Augmented Generation cu memorie pe termen lung
- üìö **Knowledge Base**: Stocare »ôi recuperare semanticƒÉ de informa»õii
- üåç **Multilingual**: Suport nativ pentru rom√¢nƒÉ »ôi englezƒÉ

### 1.2 Principii de Design

- **Modularitate**: Componente independente »ôi reutilizabile
- **Privacy-First**: Toate procesƒÉrile ruleazƒÉ local
- **Production-Ready**: Cod testat cu logging complet
- **Extensibilitate**: U»ôor de adƒÉugat noi func»õionalitƒÉ»õi

### 1.3 Status Actual

| Modul | Status | Linii Cod | Commit |
|-------|--------|-----------|--------|
| ML Core | ‚úÖ Complete | ~3,000 | `5440b02` |
| Training Pipeline | ‚úÖ Complete | ~2,000 | `bd73a7d` |
| Validation & Metrics | ‚úÖ Complete | ~1,500 | `f488ebb` |
| Data Pipeline | ‚úÖ Complete | ~1,200 | `97e31e8` |
| Inference Engine | ‚úÖ Complete | ~2,500 | `5440b02` |
| Voice Module | ‚úÖ Complete | ~350 | `01ed670` |
| RAG System | ‚úÖ Complete | ~2,400 | `650b1be` |
| **TOTAL** | **‚úÖ Functional** | **~13,000+** | 16 commits |

---

## 2. Arhitectura Sistemului

### 2.1 Structura de Fi»ôiere

```
Nova_20/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ml/                    # Machine Learning Core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attention.py       # Multi-Head Attention
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer.py     # Transformer Encoder/Decoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py      # Token & Positional Embeddings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ffn.py             # Feed-Forward Networks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py           # Complete NOVA Model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference.py       # Inference Engine + KV Cache
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sampling.py        # Generation Strategies
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/              # Training Infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py         # Main Training Loop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Training Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py       # Learning Rate Scheduling
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ validation/            # Validation & Metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py         # Evaluation Metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py       # Model Evaluation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                  # Data Pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py         # Dataset Classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py   # Data Preprocessing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.py       # Tokenization
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag/                   # RAG System (NEW!)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py      # Multi-strategy Embeddings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py    # ChromaDB Integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py         # Document Chunking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py       # Semantic Search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory.py          # Memory Management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py    # RAG Orchestration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ voice/                 # Voice Module (NEW!)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tts.py             # Text-to-Speech
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ config/                # Configuration
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ model_config.py
‚îÇ
‚îú‚îÄ‚îÄ examples/                  # Demo Scripts
‚îÇ   ‚îú‚îÄ‚îÄ voice_demo.py          # Voice Demonstrations
‚îÇ   ‚îî‚îÄ‚îÄ rag_demo.py            # RAG Demonstrations
‚îÇ
‚îú‚îÄ‚îÄ tests/                     # Test Suite
‚îÇ   ‚îú‚îÄ‚îÄ test_ml/
‚îÇ   ‚îú‚îÄ‚îÄ test_training/
‚îÇ   ‚îî‚îÄ‚îÄ test_rag/
‚îÇ
‚îú‚îÄ‚îÄ Documentation/             # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ arhitectura_nova.md   # Architecture
‚îÇ   ‚îú‚îÄ‚îÄ NOVA_MANUAL.md        # This Manual
‚îÇ   ‚îî‚îÄ‚îÄ implementation.md     # Old Implementation Plan
‚îÇ
‚îî‚îÄ‚îÄ README.md                  # Project Overview
```

### 2.2 Diagrama de ArhitecturƒÉ

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        NOVA System                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  ML Core     ‚îÇ  ‚îÇ  Voice       ‚îÇ  ‚îÇ  RAG System  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Attention  ‚îÇ  ‚îÇ ‚Ä¢ TTS        ‚îÇ  ‚îÇ ‚Ä¢ Embeddings ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Transform  ‚îÇ  ‚îÇ ‚Ä¢ pyttsx3    ‚îÇ  ‚îÇ ‚Ä¢ ChromaDB   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Inference  ‚îÇ  ‚îÇ ‚Ä¢ Multilang  ‚îÇ  ‚îÇ ‚Ä¢ Memory     ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ             Training Pipeline                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Data Loading ‚Ä¢ Training Loop ‚Ä¢ Validation        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ             Inference Engine                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ KV Cache ‚Ä¢ Beam Search ‚Ä¢ Streaming               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3. Componente Implementate

### 3.1 ML Core (‚úÖ COMPLETE)

#### 3.1.1 Attention Mechanisms

**Fi»ôier**: `src/ml/attention.py`

**Func»õionalitƒÉ»õi**:
- Scaled Dot-Product Attention
- Multi-Head Attention
- Causal Masking pentru decoder
- Attention weights computation

**Exemplu de Utilizare**:
```python
from src.ml.attention import MultiHeadAttention

attention = MultiHeadAttention(
    d_model=512,
    num_heads=8,
    dropout=0.1
)

# Forward pass
output, attention_weights = attention(query, key, value, mask)
```

#### 3.1.2 Transformer Architecture

**Fi»ôier**: `src/ml/transformer.py`

**Clase**:
- `TransformerEncoderLayer`: Single encoder layer
- `TransformerDecoderLayer`: Single decoder layer
- `TransformerEncoder`: Stack of encoder layers
- `TransformerDecoder`: Stack of decoder layers

**Exemplu**:
```python
from src.ml.transformer import TransformerEncoder

encoder = TransformerEncoder(
    num_layers=6,
    d_model=512,
    num_heads=8,
    d_ff=2048
)

encoded = encoder(input_embeddings, mask)
```

#### 3.1.3 Embeddings

**Fi»ôier**: `src/ml/embeddings.py`

**Tipuri**:
- Token Embeddings
- Sinusoidal Positional Encoding
- Learned Positional Encoding

**Exemplu**:
```python
from src.ml.embeddings import TokenEmbedding, PositionalEncoding

token_emb = TokenEmbedding(vocab_size=50000, d_model=512)
pos_enc = PositionalEncoding(d_model=512, max_len=5000)

# Combined embeddings
embeddings = pos_enc(token_emb(tokens))
```

#### 3.1.4 Complete Model

**Fi»ôier**: `src/ml/model.py`

**Clasa**: `NovaModel` - Complete transformer model

**Exemplu**:
```python
from src.ml.model import NovaModel
from src.config import NovaConfig

config = NovaConfig(
    vocab_size=50000,
    d_model=512,
    num_layers=6,
    num_heads=8
)

model = NovaModel(config)
output = model(input_ids, target_ids)
```

### 3.2 Training Pipeline (‚úÖ COMPLETE)

#### 3.2.1 Trainer

**Fi»ôier**: `src/training/trainer.py`

**Func»õionalitƒÉ»õi**:
- Training loop complet
- Mixed precision training (AMP)
- Gradient accumulation
- Checkpointing
- TensorBoard logging

**Exemplu**:
```python
from src.training.trainer import NovaTrainer

trainer = NovaTrainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    config=training_config
)

# Train model
trainer.train(num_epochs=10)
```

#### 3.2.2 Configuration

**Fi»ôier**: `src/training/config.py`

**Clasa**: `TrainingConfig`

**Parametri**:
- Learning rate: 1e-4
- Batch size: 32
- Gradient accumulation: 4
- Warmup steps: 1000
- Max grad norm: 1.0

### 3.3 Validation & Metrics (‚úÖ COMPLETE)

**Fi»ôier**: `src/validation/metrics.py`

**Metrici Disponibile**:
- Perplexity
- BLEU Score
- ROUGE Scores (ROUGE-1, ROUGE-2, ROUGE-L)
- Accuracy
- F1 Score
- Token-level metrics

**Exemplu**:
```python
from src.validation.metrics import compute_perplexity, compute_bleu

perplexity = compute_perplexity(model, val_loader)
bleu = compute_bleu(predictions, references)
```

### 3.4 Inference Engine (‚úÖ COMPLETE)

**Fi»ôier**: `src/ml/inference.py`

**Func»õionalitƒÉ»õi**:
- KV Cache pentru optimizare
- Beam Search
- Top-K Sampling
- Top-P (Nucleus) Sampling
- Temperature control
- Streaming generation

**Exemplu de Utilizare**:
```python
from src.ml.inference import NovaInference

inference = NovaInference(model, tokenizer)

# Generate text
output = inference.generate(
    prompt="Hello NOVA,",
    max_length=100,
    temperature=0.8,
    top_k=50,
    top_p=0.9
)

print(output)
```

**Strategii de Generare**:

1. **Greedy Search**: Alege mereu token-ul cu probabilitatea cea mai mare
2. **Beam Search**: ExploreazƒÉ multiple cƒÉi √Æn paralel
3. **Top-K Sampling**: Sampling din top-k tokeni cei mai probabili
4. **Top-P Sampling**: Sampling din tokeni cu probabilitate cumulatƒÉ p
5. **Temperature**: ControleazƒÉ "creativitatea" modelului

### 3.5 Voice Module (‚úÖ COMPLETE)

**Fi»ôier**: `src/voice/tts.py`

**Clasa**: `NovaVoice`

**Func»õionalitƒÉ»õi**:
- Text-to-Speech offline (pyttsx3)
- Voce femininƒÉ elegantƒÉ
- Suport multilingv (rom√¢nƒÉ + englezƒÉ)
- Control rate, volume, voice
- Context-aware intonation

**Exemplu**:
```python
from src.voice.tts import NovaVoice

voice = NovaVoice()

# Simple speech
voice.speak("Hello! I am NOVA.")

# Romanian speech
voice.speak("BunƒÉ! Sunt NOVA, asistentul tƒÉu AI.")

# Customize voice
voice.speak("Important message!", rate=120, volume=1.0)
```

**Demo Script**: `examples/voice_demo.py`

```bash
python examples/voice_demo.py
```

**5 Scenarii Demo**:
1. Basic TTS
2. Limba Rom√¢nƒÉ
3. Limba EnglezƒÉ
4. Voice Customization
5. Context-aware Speech

### 3.6 RAG System (‚úÖ COMPLETE)

#### 3.6.1 Overview

RAG (Retrieval-Augmented Generation) oferƒÉ NOVA memorie pe termen lung »ôi capacitatea de a recupera informa»õii relevante din knowledge base.

**Total Code**: ~2,400 linii  
**Commit**: `650b1be`

#### 3.6.2 Embeddings

**Fi»ôier**: `src/rag/embeddings.py`

**3 Strategii**:

1. **SentenceTransformerEmbeddings**: Pre-trained multilingual
   ```python
   from src.rag.embeddings import SentenceTransformerEmbeddings
   
   embedder = SentenceTransformerEmbeddings(
       model_name="paraphrase-multilingual-MiniLM-L12-v2"
   )
   
   embeddings = embedder.embed("Text to embed")
   ```

2. **NovaEmbeddings**: Custom folosind NOVA transformer
   ```python
   from src.rag.embeddings import NovaEmbeddings
   
   embedder = NovaEmbeddings(nova_model, tokenizer)
   embeddings = embedder.embed("Text to embed")
   ```

3. **HybridEmbeddings**: Combina»õie weighted sau concatenated
   ```python
   from src.rag.embeddings import HybridEmbeddings
   
   embedder = HybridEmbeddings(
       sentence_embedder,
       nova_embedder,
       weight_sentence=0.7
   )
   ```

#### 3.6.3 Vector Store

**Fi»ôier**: `src/rag/vector_store.py`

**Clase**:
- `ChromaVectorStore`: Persistent vector database
- `MultiCollectionStore`: Multiple collections

**Exemplu**:
```python
from src.rag.vector_store import ChromaVectorStore

store = ChromaVectorStore(
    collection_name="nova_knowledge",
    persist_directory="./chroma_db"
)

# Add documents
store.add(
    documents=["NOVA is an AI assistant"],
    embeddings=embeddings,
    metadatas=[{"source": "about"}]
)

# Search
results = store.search(
    query_embedding=query_emb,
    n_results=5
)
```

#### 3.6.4 Document Chunking

**Fi»ôier**: `src/rag/chunker.py`

**5 Strategii**:

1. **Fixed**: Fixed-size chunks cu overlap
2. **Sentence**: Sentence-boundary aware
3. **Paragraph**: PreservƒÉ paragrafele
4. **Code**: Function/class aware (Python, JS, C, Java)
5. **Smart**: Auto-detect text type

**Exemplu**:
```python
from src.rag.chunker import DocumentChunker

chunker = DocumentChunker(
    chunk_size=500,
    chunk_overlap=50,
    strategy='smart'
)

# Chunk text
chunks = chunker.chunk_text(long_text)

# Chunk file (PDF, txt, md, py, etc.)
chunks = chunker.chunk_file("document.pdf")
```

#### 3.6.5 Semantic Retrieval

**Fi»ôier**: `src/rag/retriever.py`

**Func»õionalitƒÉ»õi**:
- Semantic search
- Re-ranking (vector + term overlap)
- MMR (Maximal Marginal Relevance) diversity
- Metadata filtering
- Context-aware retrieval

**Exemplu**:
```python
from src.rag.retriever import SemanticRetriever

retriever = SemanticRetriever(
    vector_store=store,
    embeddings=embedder,
    rerank=True,
    diversity_weight=0.3
)

# Retrieve documents
results = retriever.retrieve(
    query="What is NOVA?",
    n_results=5,
    filters={"source": "documentation"}
)
```

#### 3.6.6 Memory Management

**Fi»ôier**: `src/rag/memory.py`

**3 Tipuri de Memorie**:

1. **ConversationMemory**: Short-term chat history
   ```python
   from src.rag.memory import ConversationMemory
   
   memory = ConversationMemory(max_messages=10)
   memory.add_message("user", "Hello!")
   memory.add_message("assistant", "Hi! How can I help?")
   
   history = memory.get_history()
   ```

2. **KnowledgeMemory**: Long-term vector storage
   ```python
   from src.rag.memory import KnowledgeMemory
   
   memory = KnowledgeMemory(store, embedder, chunker)
   memory.add_knowledge("NOVA can speak Romanian and English")
   
   results = memory.search_knowledge("languages")
   ```

3. **WorkingMemory**: Context assembly
   ```python
   from src.rag.memory import WorkingMemory
   
   memory = WorkingMemory(max_context_length=4000)
   context = memory.build_context(
       query="Tell me about NOVA",
       conversation_history=conv_history,
       retrieved_knowledge=knowledge_results
   )
   ```

#### 3.6.7 RAG Pipeline

**Fi»ôier**: `src/rag/rag_pipeline.py`

**Clasa PrincipalƒÉ**: `RAGPipeline`

**Func»õionalitƒÉ»õi Complete**:
- Document ingestion (text + files)
- Semantic search cu re-ranking
- Conversation tracking
- Context building pentru generation
- System statistics

**Exemplu Complet**:
```python
from src.rag.rag_pipeline import RAGPipeline

# Initialize pipeline
rag = RAGPipeline(
    collection_name="my_knowledge",
    persist_directory="./chroma_db"
)

# Add knowledge
rag.add_document(
    "NOVA is an intelligent AI assistant with voice and memory.",
    source="about_nova"
)

# Add PDF file
rag.add_file("research_paper.pdf")

# Query with context
result = rag.query(
    "What can NOVA do?",
    n_results=3,
    use_conversation_history=True
)

print(result['context'])
print(result['sources'])

# Chat interface
context = rag.chat("Tell me about transformers")
# ... generate response ...
rag.add_assistant_response("Transformers are neural networks...")

# System stats
stats = rag.get_stats()
print(f"Documents: {stats['total_documents']}")
print(f"Conversations: {stats['conversation_messages']}")
```

**Demo Script**: `examples/rag_demo.py`

```bash
python examples/rag_demo.py
```

**6 Demo Scenarios**:
1. Basic Knowledge & Retrieval
2. Multilingual Knowledge (rom√¢nƒÉ + englezƒÉ)
3. Conversation Memory
4. Complete Chat Flow
5. Advanced Retrieval (re-ranking + diversity)
6. File Ingestion (PDF, txt, etc.)

---

## 4. Ghid de Instalare

### 4.1 Cerin»õe de Sistem

- **Python**: 3.11+
- **OS**: macOS, Linux, Windows
- **RAM**: 8GB minimum (16GB recomandat)
- **Storage**: 10GB pentru dependencies + models

### 4.2 Instalare Pas cu Pas

#### Pas 1: Clone Repository

```bash
git clone https://github.com/Cezarovsky/NOVA_20.git
cd Nova_20
```

#### Pas 2: Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# sau
venv\Scripts\activate  # Windows
```

#### Pas 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies Principale**:
- `torch>=2.0.0`: Deep learning framework
- `numpy`: Numerical computing
- `tqdm`: Progress bars
- `tensorboard`: Training visualization
- `chromadb==1.3.5`: Vector database
- `sentence-transformers==5.1.2`: Pre-trained embeddings
- `pypdf2==3.0.1`: PDF processing
- `pyttsx3==2.90`: Text-to-speech
- `tiktoken==0.12.0`: Token counting

#### Pas 4: Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import chromadb; print('ChromaDB: OK')"
python -c "import pyttsx3; print('Voice: OK')"
```

### 4.3 Configuration

Nu sunt necesare API keys - totul ruleazƒÉ local!

**Optional**: ConfigurƒÉri √Æn `src/config/model_config.py`

---

## 5. Ghid de Utilizare

### 5.1 Quick Start: Generate Text

```python
from src.ml.model import NovaModel
from src.ml.inference import NovaInference
from src.config import NovaConfig

# Create model
config = NovaConfig()
model = NovaModel(config)

# Load weights (if available)
# model.load_state_dict(torch.load('nova_model.pt'))

# Create inference engine
inference = NovaInference(model, tokenizer)

# Generate text
output = inference.generate(
    prompt="Once upon a time",
    max_length=100,
    temperature=0.8
)

print(output)
```

### 5.2 Quick Start: Voice

```python
from src.voice.tts import NovaVoice

voice = NovaVoice()

# English
voice.speak("Hello! I am NOVA, your AI assistant.")

# Romanian
voice.speak("BunƒÉ! Sunt NOVA, asistentul tƒÉu inteligent.")
```

### 5.3 Quick Start: RAG System

```python
from src.rag.rag_pipeline import RAGPipeline

# Initialize
rag = RAGPipeline(collection_name="my_knowledge")

# Add knowledge
rag.add_document("NOVA can speak and remember information.")

# Query
result = rag.query("What can NOVA do?")
print(result['context'])

# Chat
context = rag.chat("Tell me more about NOVA")
```

### 5.4 Complete Demo

```bash
# Run complete NOVA demo
python run_nova.py
```

Acest script demonstreazƒÉ:
1. Model initialization
2. Text generation
3. Voice synthesis
4. RAG capabilities

---

## 6. API Reference

### 6.1 NovaModel

```python
class NovaModel(nn.Module):
    def __init__(self, config: NovaConfig)
    def forward(self, src: Tensor, tgt: Tensor) -> Tensor
    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor
    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor) -> Tensor
```

### 6.2 NovaInference

```python
class NovaInference:
    def __init__(self, model: NovaModel, tokenizer)
    
    def generate(
        self,
        prompt: str,
        max_length: int = 100,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        num_beams: int = 1
    ) -> str
    
    def generate_stream(
        self,
        prompt: str,
        max_length: int = 100,
        temperature: float = 1.0
    ) -> Iterator[str]
```

### 6.3 NovaVoice

```python
class NovaVoice:
    def __init__(self)
    
    def speak(
        self,
        text: str,
        rate: int = 150,
        volume: float = 0.9,
        voice_id: Optional[int] = None
    ) -> bool
    
    def speak_with_context(
        self,
        text: str,
        context_type: str = 'normal'
    ) -> bool
    
    def list_voices(self) -> List[Dict]
```

### 6.4 RAGPipeline

```python
class RAGPipeline:
    def __init__(
        self,
        embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2",
        collection_name: str = "nova_knowledge",
        persist_directory: Optional[str] = None
    )
    
    def add_document(
        self,
        text: str,
        source: str = "manual_input",
        metadata: Optional[Dict] = None
    ) -> List[str]
    
    def add_file(
        self,
        file_path: str,
        metadata: Optional[Dict] = None
    ) -> List[str]
    
    def query(
        self,
        question: str,
        n_results: int = 5,
        use_conversation_history: bool = True,
        return_sources: bool = True
    ) -> Dict
    
    def chat(
        self,
        user_message: str,
        n_results: int = 3
    ) -> str
    
    def get_stats(self) -> Dict
```

---

## 7. Exemple Practice

### 7.1 Training Loop Complet

```python
from src.ml.model import NovaModel
from src.training.trainer import NovaTrainer
from src.training.config import TrainingConfig
from src.data.dataset import TextDataset

# Configuration
config = TrainingConfig(
    learning_rate=1e-4,
    batch_size=32,
    num_epochs=10,
    warmup_steps=1000
)

# Model
model = NovaModel(model_config)

# Data
train_dataset = TextDataset("train.txt", tokenizer)
val_dataset = TextDataset("val.txt", tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32)
val_loader = DataLoader(val_dataset, batch_size=32)

# Trainer
trainer = NovaTrainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    config=config
)

# Train
trainer.train(num_epochs=10)

# Save model
torch.save(model.state_dict(), 'nova_trained.pt')
```

### 7.2 Inference cu Toate Strategiile

```python
from src.ml.inference import NovaInference

inference = NovaInference(model, tokenizer)

prompt = "Artificial intelligence is"

# Greedy
print("Greedy:", inference.generate(prompt, num_beams=1))

# Beam Search
print("Beam:", inference.generate(prompt, num_beams=5))

# Top-K
print("Top-K:", inference.generate(prompt, top_k=50))

# Top-P
print("Top-P:", inference.generate(prompt, top_p=0.9))

# Creative (high temperature)
print("Creative:", inference.generate(prompt, temperature=1.5))

# Conservative (low temperature)
print("Conservative:", inference.generate(prompt, temperature=0.3))
```

### 7.3 Voice + RAG Integration

```python
from src.voice.tts import NovaVoice
from src.rag.rag_pipeline import RAGPipeline

# Initialize
voice = NovaVoice()
rag = RAGPipeline()

# Add knowledge
rag.add_document("NOVA can speak in Romanian and English.")

# User query
user_query = "Ce limbi vorbe»ôti?"

# Get context from RAG
result = rag.query(user_query, n_results=3)
context = result['context']

# Generate response (simplified - normalmente folose»ôti model)
response = "Pot vorbi √Æn rom√¢nƒÉ »ôi englezƒÉ!"

# Update conversation
rag.add_assistant_response(response)

# Speak response
voice.speak(response)

print(f"Context used:\n{context}")
```

### 7.4 Complete Conversational System

```python
from src.ml.inference import NovaInference
from src.voice.tts import NovaVoice
from src.rag.rag_pipeline import RAGPipeline

class NovaAssistant:
    def __init__(self):
        self.model = NovaModel(config)
        self.inference = NovaInference(self.model, tokenizer)
        self.voice = NovaVoice()
        self.rag = RAGPipeline()
    
    def chat(self, user_input: str, speak: bool = True) -> str:
        # Get RAG context
        context = self.rag.chat(user_input)
        
        # Generate response using context
        response = self.inference.generate(
            prompt=context,
            max_length=150,
            temperature=0.8
        )
        
        # Update RAG memory
        self.rag.add_assistant_response(response)
        
        # Speak if requested
        if speak:
            self.voice.speak(response)
        
        return response

# Usage
nova = NovaAssistant()

# Add knowledge
nova.rag.add_document("NOVA was built by Cezar with love.")

# Chat
response = nova.chat("Who built you?", speak=True)
print(response)
```

---

## 8. Depanare »ôi Troubleshooting

### 8.1 Probleme Comune

#### Error: "CUDA out of memory"

**Solu»õie**:
```python
# Reduce batch size
config.batch_size = 16  # instead of 32

# Use gradient accumulation
config.gradient_accumulation_steps = 4

# Enable gradient checkpointing
model.gradient_checkpointing_enable()
```

#### Error: "ChromaDB collection already exists"

**Solu»õie**:
```python
# Reset collection
store.reset()

# Or use different name
rag = RAGPipeline(collection_name="nova_knowledge_v2")
```

#### Error: "pyttsx3 not initialized"

**Solu»õie**:
```bash
# macOS
brew install espeak

# Linux
sudo apt-get install espeak

# Windows - pyttsx3 should work out of the box
```

### 8.2 Performance Issues

#### Inference prea lent

**OptimizƒÉri**:
1. Enable KV Cache (already enabled)
2. Use mixed precision:
   ```python
   with torch.cuda.amp.autocast():
       output = model(input)
   ```
3. Reduce max_length
4. Use greedy instead of beam search

#### RAG search lent

**OptimizƒÉri**:
1. Reduce n_results
2. Disable re-ranking for simple queries
3. Use smaller embedding model
4. Create separate collections for different domains

### 8.3 Debugging

**Enable Verbose Logging**:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Check Model Parameters**:
```python
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total: {total_params:,} | Trainable: {trainable_params:,}")
```

**Validate Data**:
```python
# Check batch
batch = next(iter(train_loader))
print(f"Input shape: {batch['input'].shape}")
print(f"Target shape: {batch['target'].shape}")
```

---

## 9. Dezvoltare Viitoare

### 9.1 Roadmap

#### Q1 2025: Core Improvements
- [ ] Fine-tune NOVA pe date custom
- [ ] Optimize inference speed (ONNX/TensorRT)
- [ ] Add more embedding strategies
- [ ] Improve chunking pentru code

#### Q2 2025: Advanced Features
- [ ] Multi-modal RAG (images + text)
- [ ] Graph RAG pentru knowledge graphs
- [ ] Real-time learning din conversa»õii
- [ ] Plugin system pentru extensii

#### Q3 2025: Production
- [ ] Web interface (Streamlit/Gradio)
- [ ] REST API
- [ ] Docker deployment
- [ ] Cloud deployment (AWS/Azure)

### 9.2 Idei de Features

1. **Multi-Agent System**: Agen»õi specializa»õi coordona»õi de orchestrator
2. **Vision Capabilities**: Image understanding cu CLIP/BLIP
3. **Audio Processing**: Whisper pentru speech-to-text
4. **Web Search**: Integrare cu web search APIs
5. **Code Generation**: Specialized code assistant
6. **Document Analysis**: Advanced PDF/DOCX processing

### 9.3 Cum sƒÉ Contribui

**Pentru a adƒÉuga un feature nou**:

1. CreeazƒÉ branch nou:
   ```bash
   git checkout -b feature/new-feature
   ```

2. ImplementeazƒÉ feature-ul √Æn `src/`

3. AdaugƒÉ tests √Æn `tests/`

4. RuleazƒÉ tests:
   ```bash
   pytest tests/ -v
   ```

5. Update documentation

6. Commit »ôi push:
   ```bash
   git add .
   git commit -m "Add new feature: description"
   git push origin feature/new-feature
   ```

7. CreeazƒÉ Pull Request pe GitHub

---

## 10. Resurse Suplimentare

### 10.1 Documenta»õie

- **Architecture**: `Documentation/arhitectura_nova.md`
- **RAG Details**: `RAG_IMPLEMENTATION.md`
- **README**: `README.md`

### 10.2 Papers & References

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - https://arxiv.org/abs/1706.03762

2. **RAG: Retrieval-Augmented Generation** (Lewis et al., 2020)
   - https://arxiv.org/abs/2005.11401

3. **Sentence-BERT** (Reimers et al., 2019)
   - https://arxiv.org/abs/1908.10084

### 10.3 Tools & Libraries

- **PyTorch**: https://pytorch.org/
- **Hugging Face**: https://huggingface.co/
- **ChromaDB**: https://www.trychroma.com/
- **Sentence Transformers**: https://www.sbert.net/

### 10.4 Contact & Support

- **GitHub**: https://github.com/Cezarovsky/NOVA_20
- **Issues**: https://github.com/Cezarovsky/NOVA_20/issues

---

## Appendix A: Complete Example Script

```python
#!/usr/bin/env python3
"""
Complete NOVA demonstration script
Shows all major capabilities
"""

import torch
from src.ml.model import NovaModel
from src.ml.inference import NovaInference
from src.voice.tts import NovaVoice
from src.rag.rag_pipeline import RAGPipeline
from src.config import NovaConfig

def main():
    print("üöÄ NOVA Complete Demo\n")
    
    # 1. Initialize Components
    print("1. Initializing components...")
    config = NovaConfig()
    model = NovaModel(config)
    voice = NovaVoice()
    rag = RAGPipeline(collection_name="demo")
    
    # 2. Add Knowledge to RAG
    print("\n2. Adding knowledge to RAG...")
    rag.add_document(
        "NOVA is an AI assistant with transformer architecture, "
        "voice capabilities, and long-term memory through RAG.",
        source="about_nova"
    )
    
    # 3. Voice Demo
    print("\n3. Voice demonstration...")
    voice.speak("Hello! I am NOVA. I can speak in English and Romanian.")
    voice.speak("BunƒÉ! Pot vorbi »ôi √Æn limba rom√¢nƒÉ!")
    
    # 4. RAG Query
    print("\n4. Querying RAG system...")
    result = rag.query("What is NOVA?")
    print(f"Context retrieved:\n{result['context'][:200]}...")
    
    # 5. Text Generation (if model trained)
    print("\n5. Text generation demo...")
    # Note: Requires trained model
    # inference = NovaInference(model, tokenizer)
    # output = inference.generate("NOVA is", max_length=50)
    # print(f"Generated: {output}")
    
    # 6. Complete Chat Flow
    print("\n6. Complete chat flow...")
    user_query = "Tell me about yourself"
    context = rag.chat(user_query)
    
    # Simulate response (normally from model)
    response = "I am NOVA, an AI assistant with voice and memory!"
    rag.add_assistant_response(response)
    voice.speak(response)
    
    # 7. Stats
    print("\n7. System statistics...")
    stats = rag.get_stats()
    print(f"Documents in knowledge base: {stats['total_documents']}")
    print(f"Conversation messages: {stats['conversation_messages']}")
    
    print("\n‚úÖ Demo complete!")

if __name__ == "__main__":
    main()
```

---

**¬© 2024 NOVA AI System - Built with ‚ù§Ô∏è by Cezar**

**Version**: 0.5.0-beta  
**Last Updated**: December 3, 2024  
**License**: Educational & Research Use

üåü **NOVA can now think, speak, and remember!** üåü
