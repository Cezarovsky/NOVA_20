# Curs NOVA: De la Teorie la PracticÄƒ

**Versiune**: 1.0  
**Data**: 3 Decembrie 2024  
**Autor**: NOVA Development Team

---

## Cuprins

1. [Fundamente Matematice](#1-fundamente-matematice)
2. [Arhitectura Transformer](#2-arhitectura-transformer)
3. [Training È™i Optimizare](#3-training-È™i-optimizare)
4. [Inference È™i Generare](#4-inference-È™i-generare)
5. [RAG È™i Memory Systems](#5-rag-È™i-memory-systems)
6. [Proiect Final](#6-proiect-final)

---

## 1. Fundamente Matematice

### 1.1 Vectori È™i Embeddings

**Teorie**: Text â†’ Numere (vectori Ã®n spaÈ›iu N-dimensional)

**MatematicÄƒ**:
```
Embedding: word â†’ vector âˆˆ â„áµˆ
"cat" â†’ [0.2, -0.5, 0.8, ..., 0.1]  (d=512)

Similaritate Cosine:
sim(A,B) = (AÂ·B) / (||A|| Ã— ||B||)
```

**PracticÄƒ NOVA**:
```python
from src.ml.embeddings import NovaEmbeddings

# Create embeddings
embeddings = NovaEmbeddings(d_model=512, vocab_size=50000)

# Convert word IDs to vectors
word_ids = torch.tensor([42, 137, 891])  # [cat, is, cute]
vectors = embeddings(word_ids)  # Shape: (3, 512)

# Compute similarity
sim = torch.cosine_similarity(vectors[0], vectors[1], dim=0)
```

**ExerciÈ›iu**: CalculeazÄƒ similaritatea Ã®ntre "NOVA" È™i "AI assistant"

---

### 1.2 Matrici È™i TransformÄƒri Liniare

**Teorie**: Matricile transformÄƒ spaÈ›ii vectoriale

**MatematicÄƒ**:
```
Y = XW + b
X: input (batch, seq_len, d_model)
W: weight matrix (d_model, d_model)
Y: output (batch, seq_len, d_model)
```

**PracticÄƒ NOVA**:
```python
import torch.nn as nn

# Linear transformation in NOVA
linear = nn.Linear(512, 512)

# Forward pass
x = torch.randn(2, 10, 512)  # batch=2, seq=10, dim=512
y = linear(x)  # Same shape, transformed space
```

**ExerciÈ›iu**: ImplementeazÄƒ o matrice de proiecÈ›ie Q, K, V

---

### 1.3 FuncÈ›ii de Activare

**Teorie**: Introducerea non-linearitÄƒÈ›ii

**MatematicÄƒ**:
```
ReLU(x) = max(0, x)
GELU(x) â‰ˆ xÂ·Î¦(x)  (Gaussian Error Linear Unit)
Softmax(x)áµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
```

**PracticÄƒ NOVA**:
```python
import torch.nn.functional as F

x = torch.tensor([-2, -1, 0, 1, 2])

# ReLU
relu_output = F.relu(x)  # [0, 0, 0, 1, 2]

# GELU (used in NOVA)
gelu_output = F.gelu(x)

# Softmax (for attention)
probs = F.softmax(x, dim=0)  # Sums to 1.0
```

**ExerciÈ›iu**: Plot GELU vs ReLU pentru x âˆˆ [-5, 5]

---

## 2. Arhitectura Transformer

### 2.1 Self-Attention Mechanism

**Teorie**: Fiecare token "vede" relaÈ›ia cu toate celelalte

**MatematicÄƒ**:
```
Q = XWq, K = XWk, V = XWv

Attention(Q,K,V) = softmax(QKáµ€/âˆšdâ‚–)V

Exemplu:
Query:  "cat"  â†’ ce caut?
Key:    "cute" â†’ ce ofer?
Value:  "cute" â†’ ce informaÈ›ie am?

Score = QÂ·Káµ€ / âˆš512 = mÄƒsurÄƒ de relevanÈ›Äƒ
```

**PracticÄƒ NOVA**:
```python
from src.ml.attention import MultiHeadAttention

# Initialize attention (8 heads, 512 dim)
attention = MultiHeadAttention(
    d_model=512,
    num_heads=8,
    dropout=0.1
)

# Input sequence
x = torch.randn(2, 10, 512)  # batch=2, seq=10

# Self-attention
output, attn_weights = attention(x, x, x)

# Visualize attention
# attn_weights: (2, 8, 10, 10)
# [batch, heads, query_pos, key_pos]
```

**ExerciÈ›iu**: CalculeazÄƒ attention scores pentru "NOVA is smart"

---

### 2.2 Multi-Head Attention

**Teorie**: ParalelizÄƒm atenÈ›ia pe mai multe "subspaÈ›ii"

**MatematicÄƒ**:
```
headáµ¢ = Attention(QWqâ±, KWkâ±, VWvâ±)
MultiHead = Concat(headâ‚,...,headâ‚•)Wâ‚’

h = 8 heads
dâ‚– = d_model / h = 512 / 8 = 64 per head
```

**Vizualizare**:
```
Input (512 dim)
    â†“
Split into 8 heads (64 dim each)
    â†“
[Head1] [Head2] ... [Head8]
  â†“       â†“           â†“
Attention Attention Attention
  â†“       â†“           â†“
Concat all heads
    â†“
Output projection (512 dim)
```

**PracticÄƒ NOVA**:
```python
# Already implemented in attention layer above
# Each head learns different patterns:
# - Head 1: syntax (subject-verb)
# - Head 2: semantics (word meanings)
# - Head 3: long-range dependencies
# etc.
```

**ExerciÈ›iu**: VizualizeazÄƒ cele 8 heads pentru o propoziÈ›ie

---

### 2.3 Positional Encoding

**Teorie**: InjectÄƒm informaÈ›ie despre poziÈ›ie (Transformers nu au ordine nativÄƒ)

**MatematicÄƒ**:
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

pos = poziÈ›ia Ã®n secvenÈ›Äƒ (0, 1, 2, ...)
i = dimensiunea (0 la d_model/2)
```

**PracticÄƒ NOVA**:
```python
from src.ml.embeddings import PositionalEncoding

# Create positional encoding
pos_enc = PositionalEncoding(d_model=512, max_len=5000)

# Apply to embeddings
embedded = embeddings(word_ids)  # (batch, seq, 512)
embedded_with_pos = pos_enc(embedded)

# Positional patterns are added automatically
```

**ExerciÈ›iu**: Plot PE pentru primele 50 poziÈ›ii È™i 512 dimensiuni

---

### 2.4 Feed-Forward Network

**Teorie**: MLP care proceseazÄƒ fiecare poziÈ›ie independent

**MatematicÄƒ**:
```
FFN(x) = GELU(xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚

d_model = 512 â†’ d_ff = 2048 â†’ d_model = 512
Expansion factor: 4x
```

**PracticÄƒ NOVA**:
```python
from src.ml.feedforward import PositionwiseFeedForward

# FFN layer
ffn = PositionwiseFeedForward(
    d_model=512,
    d_ff=2048,
    dropout=0.1
)

# Forward pass
x = torch.randn(2, 10, 512)
output = ffn(x)  # Same shape: (2, 10, 512)
```

**ExerciÈ›iu**: CalculeazÄƒ numÄƒrul de parametri Ã®n FFN

---

### 2.5 Layer Normalization

**Teorie**: StabilizeazÄƒ training-ul prin normalizare

**MatematicÄƒ**:
```
LayerNorm(x) = Î³Â·(x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²

Î¼ = mean(x)
ÏƒÂ² = variance(x)
Î³, Î² = parametri Ã®nvÄƒÈ›aÈ›i
```

**PracticÄƒ NOVA**:
```python
import torch.nn as nn

# Layer normalization
layer_norm = nn.LayerNorm(512)

x = torch.randn(2, 10, 512)
normalized = layer_norm(x)

# Properties:
# mean â‰ˆ 0, std â‰ˆ 1 for each feature dimension
```

**ExerciÈ›iu**: ComparÄƒ LayerNorm cu BatchNorm

---

### 2.6 Transformer Block Complet

**Teorie**: CombinÄƒm toate componentele

**ArhitecturÄƒ**:
```
Input
  â†“
+ MultiHeadAttention
  â†“
LayerNorm
  â†“
+ FeedForward
  â†“
LayerNorm
  â†“
Output
```

**PracticÄƒ NOVA**:
```python
from src.ml.transformer import TransformerBlock

# Single transformer block
block = TransformerBlock(
    d_model=512,
    num_heads=8,
    d_ff=2048,
    dropout=0.1
)

# Forward pass
x = torch.randn(2, 10, 512)
output = block(x, mask=None)
```

**ExerciÈ›iu**: CalculeazÄƒ receptive field dupÄƒ N blocks

---

## 3. Training È™i Optimizare

### 3.1 Loss Function

**Teorie**: Cross-Entropy pentru predicÈ›ie next token

**MatematicÄƒ**:
```
Loss = -Î£áµ¢ yáµ¢ log(Å·áµ¢)

y = one-hot true token
Å· = predicted probabilities

Example:
True: "cat" (token 42)
Pred: [0.1, 0.05, ..., 0.7, ..., 0.02]  (50k vocab)
Loss = -log(0.7) = 0.357
```

**PracticÄƒ NOVA**:
```python
import torch.nn.functional as F

# Predictions (batch=2, seq=10, vocab=50000)
logits = model(input_ids)

# True labels (shifted by 1 position)
targets = input_ids[:, 1:]  # Next token

# Compute loss
loss = F.cross_entropy(
    logits[:, :-1].reshape(-1, vocab_size),
    targets.reshape(-1)
)
```

**ExerciÈ›iu**: CalculeazÄƒ loss pentru "NOVA is [MASK]"

---

### 3.2 Adam Optimizer

**Teorie**: Adaptive learning rate per parametru

**MatematicÄƒ**:
```
mâ‚œ = Î²â‚mâ‚œâ‚‹â‚ + (1-Î²â‚)gâ‚œ         (momentum)
vâ‚œ = Î²â‚‚vâ‚œâ‚‹â‚ + (1-Î²â‚‚)gâ‚œÂ²        (variance)
Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î·Â·mâ‚œ/âˆš(vâ‚œ + Îµ)      (update)

Î²â‚ = 0.9, Î²â‚‚ = 0.999
Î· = learning rate
```

**PracticÄƒ NOVA**:
```python
from src.training.trainer import NovaTrainer

# Training configuration
config = TrainingConfig(
    learning_rate=1e-4,
    batch_size=32,
    num_epochs=10,
    warmup_steps=1000
)

trainer = NovaTrainer(model, tokenizer, config)
```

**ExerciÈ›iu**: Plot learning rate cu warmup

---

### 3.3 Learning Rate Scheduling

**Teorie**: Warmup + Decay pentru convergenÈ›Äƒ

**MatematicÄƒ**:
```
Warmup: lr(t) = lr_max Â· (t / warmup_steps)
Decay:  lr(t) = lr_max Â· âˆš(d_model) / âˆš(max(t, warmup))
```

**PracticÄƒ NOVA**:
```python
# Automatically handled by trainer
trainer.train(train_dataset)

# LR schedule:
# Steps 0-1000: Linear warmup
# Steps 1000+: Inverse sqrt decay
```

**ExerciÈ›iu**: Plot LR pentru 10k steps

---

### 3.4 Gradient Clipping

**Teorie**: Previne exploding gradients

**MatematicÄƒ**:
```
g_clipped = g Â· min(1, max_norm / ||g||)

DacÄƒ ||g|| > max_norm (ex: 1.0):
  Scale down gradient
```

**PracticÄƒ NOVA**:
```python
# In training loop
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

**ExerciÈ›iu**: MonitorizeazÄƒ grad norm pe 100 steps

---

### 3.5 Training Loop Complet

**PracticÄƒ NOVA**:
```python
from src.training.trainer import NovaTrainer
from src.data.dataset import NovaDataset

# 1. Prepare data
dataset = NovaDataset("train.txt", tokenizer, max_length=512)

# 2. Configure training
config = TrainingConfig(
    learning_rate=1e-4,
    batch_size=32,
    num_epochs=10,
    save_every=1000,
    eval_every=500
)

# 3. Train
trainer = NovaTrainer(model, tokenizer, config)
history = trainer.train(dataset)

# 4. Monitor
print(f"Final loss: {history['train_loss'][-1]:.4f}")
```

**ExerciÈ›iu**: Train pe 1000 samples, plot loss curve

---

## 4. Inference È™i Generare

### 4.1 Greedy Decoding

**Teorie**: Alege mereu token-ul cu probabilitate maximÄƒ

**MatematicÄƒ**:
```
xâ‚œ = argmax P(x|xâ‚,...,xâ‚œâ‚‹â‚)
```

**PracticÄƒ NOVA**:
```python
from src.ml.inference import NovaInference

inference = NovaInference(model, tokenizer)

# Greedy generation
output = inference.generate(
    "NOVA is",
    max_length=50,
    strategy="greedy"
)
# Output: "NOVA is an advanced AI assistant..."
```

**ExerciÈ›iu**: Generate 5 continuÄƒri pentru "The weather is"

---

### 4.2 Beam Search

**Teorie**: ExploreazÄƒ top-K cÄƒi Ã®n paralel

**MatematicÄƒ**:
```
Score(sequence) = Î£â‚œ log P(xâ‚œ|xâ‚,...,xâ‚œâ‚‹â‚)
Keep top K sequences at each step
```

**PracticÄƒ NOVA**:
```python
output = inference.generate(
    "NOVA is",
    max_length=50,
    strategy="beam_search",
    num_beams=5
)
# More diverse and coherent output
```

**ExerciÈ›iu**: ComparÄƒ greedy vs beam (5 beams)

---

### 4.3 Sampling cu TemperaturÄƒ

**Teorie**: ControleazÄƒ randomness

**MatematicÄƒ**:
```
P'(x) = softmax(logits / T)

T < 1: mai deterministÄƒ (confident)
T = 1: distribuÈ›ie originalÄƒ
T > 1: mai aleatorie (creative)
```

**PracticÄƒ NOVA**:
```python
# Conservative (T=0.5)
output1 = inference.generate(
    "Once upon a time",
    temperature=0.5,
    strategy="sampling"
)

# Creative (T=1.5)
output2 = inference.generate(
    "Once upon a time",
    temperature=1.5,
    strategy="sampling"
)
```

**ExerciÈ›iu**: Generate cu T âˆˆ {0.3, 0.7, 1.0, 1.5}, comparÄƒ

---

### 4.4 Top-K È™i Top-P (Nucleus) Sampling

**Teorie**: FiltreazÄƒ token-uri improbabile

**MatematicÄƒ**:
```
Top-K: PÄƒstreazÄƒ K token-uri cu prob. cea mai mare
Top-P: PÄƒstreazÄƒ token-uri pÃ¢nÄƒ prob. cumulativÄƒ >= P

P(nucleus) >= p (ex: 0.9)
```

**PracticÄƒ NOVA**:
```python
# Top-K sampling
output = inference.generate(
    "NOVA can",
    strategy="top_k",
    top_k=50
)

# Top-P (nucleus) sampling
output = inference.generate(
    "NOVA can",
    strategy="top_p",
    top_p=0.9
)
```

**ExerciÈ›iu**: GenereazÄƒ 10 outputs, calculeazÄƒ diversity score

---

### 4.5 KV Cache pentru ViteazÄƒ

**Teorie**: Cache-uim key/value pentru token-uri generate

**MatematicÄƒ**:
```
FÄƒrÄƒ cache: O(nÂ²) attention pentru n tokens
Cu cache:   O(n) doar pentru ultimul token

Speedup: ~10x pentru secvenÈ›e lungi
```

**PracticÄƒ NOVA**:
```python
# KV cache is automatic in NOVA inference
output = inference.generate(
    "Long prompt here...",
    max_length=500,
    use_cache=True  # Default
)
# Generates 500 tokens ~10x faster
```

**ExerciÈ›iu**: Benchmark cu/fÄƒrÄƒ cache pentru 100 tokens

---

## 5. RAG È™i Memory Systems

### 5.1 Embeddings pentru Retrieval

**Teorie**: ReprezentÄƒri dense pentru similaritate semanticÄƒ

**MatematicÄƒ**:
```
Cosine Similarity:
sim(q, d) = (qÂ·d) / (||q|| Ã— ||d||)

query: "What is NOVA?"
docs: ["NOVA is AI", "Python code", "Weather"]
scores: [0.89, 0.23, 0.15]
```

**PracticÄƒ NOVA**:
```python
from src.rag.embeddings import SentenceTransformerEmbeddings

# Initialize embedder
embedder = SentenceTransformerEmbeddings()

# Embed query and documents
query_emb = embedder.embed_query("What is NOVA?")
doc_embs = embedder.embed_documents([
    "NOVA is an AI assistant",
    "Python programming",
    "Weather forecast"
])

# Compute similarities
similarities = [
    torch.cosine_similarity(query_emb, doc_emb, dim=0)
    for doc_emb in doc_embs
]
```

**ExerciÈ›iu**: CreeazÄƒ 5 docs, find top-3 pentru query

---

### 5.2 Vector Store cu ChromaDB

**Teorie**: Database pentru search semantic rapid

**PracticÄƒ NOVA**:
```python
from src.rag.vector_store import ChromaVectorStore

# Initialize persistent store
store = ChromaVectorStore(
    collection_name="my_knowledge",
    persist_directory="./chroma_db"
)

# Add documents
docs = ["NOVA is an AI", "Python is great"]
embeddings = embedder.embed_documents(docs)
store.add(docs, embeddings)

# Search
query_emb = embedder.embed_query("Tell me about NOVA")
results = store.search(query_emb, n_results=3)
```

**ExerciÈ›iu**: Build knowledge base cu 20 facts despre NOVA

---

### 5.3 Document Chunking

**Teorie**: ÃmpÄƒrÈ›im texte lungi Ã®n fragmente semantice

**PracticÄƒ NOVA**:
```python
from src.rag.chunker import DocumentChunker

chunker = DocumentChunker(
    chunk_size=500,
    chunk_overlap=50,
    strategy='smart'
)

# Chunk a document
text = open("long_article.txt").read()
chunks = chunker.chunk_text(text)

# Each chunk: 450-550 chars with context overlap
for i, chunk in enumerate(chunks):
    print(f"Chunk {i}: {len(chunk)} chars")
```

**ExerciÈ›iu**: Chunk o carte, count chunks, plot distribution

---

### 5.4 Complete RAG Pipeline

**PracticÄƒ NOVA**:
```python
from src.rag.rag_pipeline import RAGPipeline

# Initialize RAG system
rag = RAGPipeline(collection_name="knowledge")

# Add knowledge from file
rag.add_file("documentation.pdf")

# Query with context retrieval
result = rag.query(
    "How does NOVA handle Romanian?",
    n_results=5
)

# result contains:
# - Retrieved documents
# - Assembled context
# - Source citations
```

**ExerciÈ›iu**: Build RAG system cu documentaÈ›ia NOVA, test queries

---

### 5.5 Memory-Augmented Conversations

**PracticÄƒ NOVA**:
```python
# RAG + Conversation Memory
rag = RAGPipeline(collection_name="chat")

# Turn 1
context1 = rag.chat("Who created NOVA?")
response1 = model.generate(context1)
rag.add_assistant_response(response1)

# Turn 2 (remembers context)
context2 = rag.chat("What can it do?")  # "it" = NOVA
response2 = model.generate(context2)
```

**ExerciÈ›iu**: ConversaÈ›ie multi-turn cu 5 Ã®ntrebÄƒri

---

## 6. Proiect Final

### 6.1 Build Your Own NOVA Assistant

**Obiectiv**: Chatbot cu RAG È™i Voice pentru domeniu specific

**PaÈ™i**:

1. **Prepare Knowledge Base**
```python
# Collect domain documents
docs = ["doc1.pdf", "doc2.txt", "doc3.md"]

# Build RAG system
rag = RAGPipeline("my_domain")
for doc in docs:
    rag.add_file(doc)
```

2. **Setup Voice**
```python
from src.voice.tts import NovaVoice
voice = NovaVoice()
```

3. **Create Chat Loop**
```python
while True:
    user_input = input("You: ")
    
    # Retrieve context
    context = rag.query(user_input, n_results=3)
    
    # Generate response
    response = inference.generate(
        context,
        max_length=200,
        temperature=0.7
    )
    
    # Speak response
    print(f"NOVA: {response}")
    voice.speak(response)
    
    # Update memory
    rag.add_assistant_response(response)
```

**ExerciÈ›iu Final**: CreeazÄƒ chatbot pentru un domeniu la alegere (medicinÄƒ, drept, IT, etc.)

---

## Resurse È™i Bibliografie

**Papers**:
- "Attention Is All You Need" (Vaswani et al., 2017)
- "BERT" (Devlin et al., 2018)
- "GPT-3" (Brown et al., 2020)

**NOVA Documentation**:
- `NOVA_MANUAL.md`: Complete implementation guide
- `arhitectura_nova.md`: Technical architecture
- `RAG_IMPLEMENTATION.md`: RAG system details

**Practice**:
- `examples/`: 8+ demo scripts
- `tests/`: Unit tests for all components

---

## UrmÄƒtorii PaÈ™i

1. âœ… Complete cursul teoretic
2. ğŸ”„ ImplementeazÄƒ fiecare exerciÈ›iu
3. ğŸ¯ Build proiect final
4. ğŸš€ Deploy Ã®n producÈ›ie

**Mult succes!** ğŸ“
