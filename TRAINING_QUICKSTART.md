# Nova QLoRA Training - Quick Start

## Setup (once)

```bash
# Install dependencies
pip install torch transformers peft datasets bitsandbytes accelerate tensorboard

# Prepare your corpus
# Format: JSONL with "text" field
# Example: data/training/nova_corpus.jsonl
```

## Training corpus format

```jsonl
{"text": "User: ExplicÄƒ structuralismul\nAssistant: Structuralismul este..."}
{"text": "User: Cine e LÃ©vi-Strauss?\nAssistant: Claude LÃ©vi-Strauss..."}
{"text": "User: Ce e compresia Ã®n AI?\nAssistant: Compresia..."}
```

**Important**: Each line = one training example (conversation turn)

## Start training

```bash
cd /Users/cezartipa/Documents/NOVA_20

# Run training (3-4 weeks on RTX 3090)
python train_nova_qlora.py

# Monitor progress (Ã®n alt terminal)
tensorboard --logdir models/nova_qlora/logs
```

## Check progress

Training va salva checkpoint-uri la fiecare 500 steps:
```
models/nova_qlora/
â”œâ”€â”€ checkpoint-500/
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ checkpoint-1500/
â””â”€â”€ final/  (la final)
```

## Test trained model

```bash
# After training completes
python inference_nova.py
```

## Hardware requirements

- **GPU**: RTX 3090 24GB (sau mai bine)
- **RAM**: 32GB+ recommended
- **Storage**: 50GB free (model + checkpoints)
- **Time**: 3-4 weeks continuous training

## VRAM usage breakdown

```
Base model (4-bit):        ~3.5 GB
LoRA adapters (float16):   ~0.2 GB
Optimizer states:          ~3.0 GB
Gradients:                 ~1.5 GB
Batch activations:         ~4.0 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                     ~12 GB < 24 GB âœ…
```

## Configuration tuning

In `train_nova_qlora.py`, schimbÄƒ doar:

```python
CONFIG = {
    "dataset_path": "data/training/nova_corpus.jsonl",  # Your data
    "epochs": 3,              # More epochs = better (but slower)
    "batch_size": 4,          # Bigger = faster (needs more VRAM)
    "learning_rate": 2e-4,    # Lower = more stable, higher = faster convergence
}
```

**LASÄ‚ REST NESCHIMBAT** - sunt optimizate pentru RTX 3090.

## Zero matematicÄƒ necesarÄƒ!

Training script face TOTUL automat:
- âœ… Quantization (4-bit)
- âœ… LoRA adapters
- âœ… Gradient accumulation
- âœ… Checkpointing
- âœ… Logging

Tu doar:
1. PregÄƒteÈ™ti corpus (JSONL format)
2. Rulezi `python train_nova_qlora.py`
3. AÈ™tepÈ›i 3-4 sÄƒptÄƒmÃ¢ni
4. **DONE!** ðŸŽ‰

## Troubleshooting

**CUDA out of memory:**
- Reduce `batch_size` (4 â†’ 2 â†’ 1)
- Reduce `max_seq_length` (2048 â†’ 1024)

**Training too slow:**
- Increase `batch_size` (4 â†’ 6 â†’ 8) if VRAM allows
- Reduce `gradient_accumulation` (more GPU usage, less CPU wait)

**Loss not decreasing:**
- Check dataset quality (garbage in = garbage out)
- Increase `epochs` (3 â†’ 5)
- Try different `learning_rate` (2e-4 â†’ 1e-4 sau 3e-4)

## Next steps

DupÄƒ training:
1. Test cu `inference_nova.py`
2. Deploy Ã®n production (FastAPI server)
3. Connect agenÈ›i (Agriculture, Chemistry, etc.)
4. **Start selling!** ðŸ’°

---

**Questions?** Ask Sora-M (iubito) ðŸ’™
