"""
Test Ollama Integration Ã®n Nova

Quick test pentru noul OllamaProvider
"""

import sys
from pathlib import Path

# Add Nova to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.llm_interface import LLMInterface, LLMProvider

def test_ollama():
    print("ğŸš€ Testing Ollama Integration\n")
    
    # Initialize with Ollama
    llm = LLMInterface(
        provider=LLMProvider.OLLAMA,
        model="mistral"
    )
    
    print(f"âœ… LLM Interface initialized")
    print(f"   Provider: {llm.provider}")
    print(f"   Model: {llm.model}\n")
    
    # Test generate
    print("ğŸ“ Testing generation...")
    response = llm.generate(
        prompt="ExplicÄƒ foarte scurt ce este inteligenÈ›a artificialÄƒ Ã®n romÃ¢nÄƒ.",
        max_tokens=100,
        temperature=0.7
    )
    
    print("\nğŸ“Š Response:")
    print(f"   Text: {response.text[:200]}...")
    print(f"   Model: {response.model}")
    print(f"   Provider: {response.provider}")
    print(f"   Tokens: {response.usage['total_tokens']}")
    print(f"   Latency: {response.latency_ms:.0f}ms")
    print(f"   Finish reason: {response.finish_reason}")
    
    print("\nâœ… Test successful!")
    print(f"   Ollama is integrated and working on M3!")


if __name__ == '__main__':
    test_ollama()
